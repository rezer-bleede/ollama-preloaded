# ğŸ§  Ollama Preloaded (Llama 3)

Pre-built Docker image containing **Ollama** and a **pre-cached Llama 3 model**, ready for instant startup â€” no downloads, no TLS issues, no waiting.

---

## ğŸš€ Overview

This image is optimized for developers and data engineers who need:
- **Offline or airâ€‘gapped** Ollama deployments  
- **Fast container startup** without model pulls  
- **Stable builds** that work in CI/CD pipelines  
- **Automatic publishing** to Docker Hub + GitHub Container Registry

The container is built via a twoâ€‘stage Dockerfile using the official [`ollama/ollama:latest`](https://hub.docker.com/r/ollama/ollama) base image.

---

## ğŸ§± Build Pipeline

Each push or manual run of the GitHub Action will:

1. Pull the latest `ollama/ollama:latest` base image  
2. Launch a temporary `ollama serve` daemon  
3. Preload the `llama3` model  
4. Copy preloaded models to a clean final image  
5. Push to **Docker Hub** and **GHCR** simultaneously

| Registry | Image |
|-----------|--------|
| GitHub Container Registry | `ghcr.io/rezer-bleede/ollama-preloaded:llama3` |
| Docker Hub | `rezerbleede/ollama-preloaded:llama3` |

---

## ğŸ³ Usage

### Run directly
```bash
docker run -d -p 11434:11434 ghcr.io/rezer-bleede/ollama-preloaded:llama3
```

### Check available models
```bash
docker exec -it <container_id> ollama list
```

### Generate a response
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Summarize database normalization."
}'
```

---

## âš™ï¸ Example `docker-compose.yml`

```yaml
services:
  ollama:
    image: ghcr.io/rezer-bleede/ollama-preloaded:llama3
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      OLLAMA_KEEP_ALIVE: 24h
```

---

## ğŸ§© Why This Image

- âœ… **Instant startup** â€“ no model downloads  
- ğŸ”’ **Offlineâ€‘ready** â€“ ships with `llama3` included  
- ğŸ§° **CI/CD compatible** â€“ builds cleanly in GitHub Actions  
- âš¡ **Dualâ€‘registry support** â€“ GHCR + Docker Hub autoâ€‘publish  
- ğŸ§± **Twoâ€‘stage build** â€“ no leftover files or temp daemons  

---

## ğŸ§° Local Build (Optional)

If you prefer to build locally instead of GitHub Actions:

```bash
git clone https://github.com/rezer-bleede/ollama-preloaded.git
cd ollama-preloaded
docker build -t ollama-preloaded:llama3 .
```

---

## ğŸªª License

Licensed under **Apache 2.0**.  
Ollama and LlamaÂ 3 are trademarks of their respective owners.

---

## ğŸ‘¤ Author

**RemisÂ BobbyÂ Haroon**  
Data Engineer â€¢ AI Infrastructure Builder  
[GitHubÂ @rezerâ€‘bleede](https://github.com/rezer-bleede)

---

> âš ï¸ Community-maintained image â€” not affiliated with the Ollama team.
